{"api_description": "TokenizerModel model = TokenizerModel(modelIn): class TokenizerME\t;", "api_example": "import opennlp.tools.tokenize.TokenizerME;\nimport opennlp.tools.tokenize.TokenizerModel;\nimport java.io.FileInputStream;\nimport java.io.InputStream;\n\npublic class TokenizerExample {\n\n    public static void main(String[] args) {\n        try {\n            InputStream modelIn = new FileInputStream(\"en-token.bin\");\n            TokenizerModel model = new TokenizerModel(modelIn);\n            TokenizerME tokenizer = new TokenizerME(model);\n            \n            // Use the tokenizer here\n            String[] tokens = tokenizer.tokenize(\"Example sentence to tokenize.\");\n            \n            // Print the tokens\n            for (String token : tokens) {\n                System.out.println(token);\n            }\n            \n            modelIn.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}"}
{"api_description": "Tokenizer tokenizer = new TokenizerME(model): class TokenizerME\t;", "api_example": "import opennlp.tools.tokenize.TokenizerME;\nimport opennlp.tools.tokenize.TokenizerModel;\nimport java.io.FileInputStream;\nimport java.io.IOException;\n\npublic class Main {\n    public static void main(String[] args) {\n        TokenizerModel model;\n        TokenizerME tokenizer;\n\n        try {\n            model = new TokenizerModel(new FileInputStream(\"en-token.bin\"));\n            tokenizer = new TokenizerME(model);\n\n            String[] tokens = tokenizer.tokenize(\"This is a sample sentence.\");\n            for (String token : tokens) {\n                System.out.println(token);\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}"}
{"api_description": "public class englishStemmer extends Object This class implements the stemming algorithm defined by a snowball script. Generated by Snowball (build from 867c4ec70debd4daa7fb4d5a9f7759b47887d0b9): class englishStemmer\t- https://github.com/snowballstem/snowball", "api_example": "import org.tartarus.snowball.ext.EnglishStemmer;\n\npublic class Example {\n    public static void main(String[] args) {\n        EnglishStemmer stemmer = new EnglishStemmer();\n        stemmer.setCurrent(\"running\");\n        stemmer.stem();\n        String stemmedWord = stemmer.getCurrent();\n        System.out.println(stemmedWord);\n    }\n}"}
{"api_description": "Map<String,Double> scoreMap(String[] text): class DocumentCategorizer\tRetrieves a Map in which the key is the category name and the value is the score.", "api_example": "import java.util.Map;\n\npublic class DocumentCategorizer {\n    public Map<String, Double> scoreMap(String[] text) {\n        // API usage example\n        String[] sampleText = {\"sample\", \"text\"};\n        DocumentCategorizer categorizer = new DocumentCategorizer();\n        Map<String, Double> result = categorizer.scoreMap(sampleText);\n        \n        return result;\n    }\n}"}
{"api_description": "Collection<String> extractFeatures(String[] text, Map<String,Object> extraInfo): class NGramFeatureGenerator\tExtracts features from given text fragments.", "api_example": "import java.util.Collection;\nimport java.util.Map;\n\npublic class Main {\n    public static void main(String[] args) {\n        String[] text = {\"This is a test\", \"Another test\"};\n        Map<String, Object> extraInfo = null;\n\n        Collection<String> features = extractFeatures(text, extraInfo);\n    }\n\n    public static Collection<String> extractFeatures(String[] text, Map<String, Object> extraInfo) {\n        // Implement feature extraction logic here\n        return null;\n    }\n}"}
{"api_description": "static List<Event> generateEvents(String[] sentence, String[] outcomes, NameContextGenerator cg): class NameFinderEventStream\tGenerates events for each token in a sentence with the specified outcomes using the specified NameContextGenerator.", "api_example": "import java.util.List;\n\nimport opennlp.tools.ml.model.Event;\nimport opennlp.tools.namefind.DefaultNameContextGenerator;\nimport opennlp.tools.namefind.NameContextGenerator;\nimport opennlp.tools.namefind.NameFinderEventStream;\n\npublic class EventGenerator {\n    public static void main(String[] args) {\n        String[] sentence = {\"John\", \"Smith\", \"is\", \"a\", \"software\", \"engineer\"};\n        String[] outcomes = {\"PER\", \"PER\", \"O\", \"O\", \"O\", \"O\"};\n        NameContextGenerator cg = new DefaultNameContextGenerator();\n\n        List<Event> events = NameFinderEventStream.generateEvents(sentence, outcomes, cg);\n    }\n}"}
{"api_description": "double[] categorize(String[] text, Map<String,Object> extraInformation): class DocumentCategorizer\tCategorizes the given text provided as tokens along with the provided extraInformation.", "api_example": "import java.util.Map;\n\nimport opennlp.tools.doccat.DocumentCategorizerME;\n\npublic class Example {\n    public static void main(String[] args) {\n        String[] text = {\"This\", \"is\", \"a\", \"test\"};\n        Map<String, Object> extraInformation = null;\n\n        DocumentCategorizerME categorizer = new DocumentCategorizerME(null);\n        double[] result = categorizer.categorize(text, extraInformation);\n    }\n}"}
{"api_description": "Collection<String> extractFeatures(String[] text, Map<String,Object> extraInformation): class FeatureGenerator\tExtracts features from given text fragments.", "api_example": "import java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport opennlp.tools.doccat.BagOfWordsFeatureGenerator;\nimport opennlp.tools.doccat.FeatureGenerator;\n\npublic class ExampleClass {\n    public static void main(String[] args) {\n        String[] text = {\"This is a sample text\", \"Another sample text\"};\n        Map<String, Object> extraInformation = new HashMap<>();\n        extraInformation.put(\"key1\", \"value1\");\n        extraInformation.put(\"key2\", 123);\n        \n        // FeatureGenerator, All Known Implementing Classes: BagOfWordsFeatureGenerator, NGramFeatureGenerator\n        FeatureGenerator generator = new BagOfWordsFeatureGenerator();\n        Collection<String> extractedFeatures = generator.extractFeatures(text, extraInformation);\n        \n        for (String feature : extractedFeatures) {\n            System.out.println(feature);\n        }\n    }\n}"}
{"api_description": "double[] categorize(String[] text): class DocumentCategorizer\tCategorizes the given text, provided in separate tokens.", "api_example": "import opennlp.tools.doccat.DoccatModel;\nimport opennlp.tools.doccat.DocumentCategorizer;\nimport opennlp.tools.doccat.DocumentCategorizerME;\n\npublic class Main {\n    public static void main(String[] args) {\n        String[] text = {\"example\", \"text\", \"for\", \"categorization\"};\n        DoccatModel model = null;\n        DocumentCategorizer categorizer = new DocumentCategorizerME(model);\n        double[] result = categorizer.categorize(text);\n        // Do something with the categorization result\n    }\n}"}
{"api_description": "static ParserModel train(String languageCode, ObjectStream<Parse> parseSamples, HeadRules rules, TrainingParameters mlParams): class Parser\tStarts a training of a ParserModel.", "api_example": "import java.io.FileNotFoundException;\nimport java.io.FileReader;\nimport java.io.IOException;\n\nimport opennlp.tools.parser.HeadRules;\nimport opennlp.tools.parser.Parse;\nimport opennlp.tools.parser.ParserModel;\nimport opennlp.tools.parser.treeinsert.Parser;\nimport opennlp.tools.util.ObjectStream;\nimport opennlp.tools.util.TrainingParameters;\n\npublic class ParserExample {\n    public static void main(String[] args) throws FileNotFoundException, IOException {\n        String languageCode = \"en\";\n        ObjectStream<Parse> parseSamples = null; // initialize with actual parse samples\n        HeadRules rules = new opennlp.tools.parser.lang.en.HeadRules(new FileReader(\"\")); // initialize with actual head rules\n        TrainingParameters mlParams = new TrainingParameters(); // initialize with actual machine learning parameters\n\n        try {\n\t\t\tParserModel model = Parser.train(languageCode, parseSamples, rules, mlParams);\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t}\n    }\n}"}
{"api_description": "ParserModel(String languageCode, MaxentModel buildModel, MaxentModel checkModel, POSModel parserTagger, ChunkerModel chunkerTagger, HeadRules headRules, ParserType type, Map<String,String> manifestInfoEntries): class ParserModel\tInitializes a ParserModel instance via given parameters.", "api_example": "import java.io.File;\nimport java.io.IOException;\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport opennlp.tools.chunker.ChunkerModel;\nimport opennlp.tools.ml.maxent.GISModel;\nimport opennlp.tools.ml.model.MaxentModel;\nimport opennlp.tools.parser.ParserModel;\nimport opennlp.tools.parser.ParserType;\nimport opennlp.tools.parser.lang.en.HeadRules;\nimport opennlp.tools.postag.POSModel;\n\npublic class ParserModelExample {\n\n    public static void main(String[] args) throws IOException {\n        String languageCode = \"en\";\n        MaxentModel buildModel = new GISModel(null, args, args);\n        MaxentModel checkModel = new GISModel(null, args, args);\n        POSModel parserTagger = new POSModel(new File(\"your-file-path\"));\n        ChunkerModel chunkerTagger = new ChunkerModel(new File(\"your-file-path\"));\n        HeadRules headRules = new HeadRules(null);\n        ParserType type = ParserType.CHUNKING;\n        Map<String, String> manifestInfoEntries = new HashMap<>();\n\n        ParserModel parserModel = new ParserModel(languageCode, buildModel, checkModel, parserTagger, chunkerTagger, headRules, type, manifestInfoEntries);\n    }\n}"}
{"api_description": "static SentenceModel train(String languageCode, ObjectStream<SentenceSample> samples, SentenceDetectorFactory sdFactory, TrainingParameters mlParams): class SentenceDetectorME\tStarts a training of a SentenceModel with the given parameters.", "api_example": "import opennlp.tools.sentdetect.SentenceDetectorME;\nimport opennlp.tools.sentdetect.SentenceModel;\nimport opennlp.tools.sentdetect.SentenceSample;\nimport opennlp.tools.util.ObjectStream;\nimport opennlp.tools.util.TrainingParameters;\n\npublic class SentenceDetectorExample {\n\n    public static void main(String[] args) {\n\n        // Define language code\n        String languageCode = \"en\";\n\n        // Define training data\n        ObjectStream<SentenceSample> trainingData = null; // Initialize with actual training data\n\n        // Create SentenceDetectorFactory\n        SentenceDetectorFactory sdFactory = null; // Initialize with actual SentenceDetectorFactory object\n\n        // Define machine learning parameters\n        TrainingParameters mlParams = new TrainingParameters();\n        mlParams.put(TrainingParameters.ITERATIONS_PARAM, 100);\n        mlParams.put(TrainingParameters.CUTOFF_PARAM, 5);\n\n        // Train SentenceModel\n        try {\n            SentenceModel model = SentenceDetectorME.train(languageCode, trainingData, sdFactory, mlParams);\n            // Use the trained model for sentence detection\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}"}
{"api_description": "DefaultLemmatizerContextGenerator(): class DefaultLemmatizerContextGenerator", "api_example": "import opennlp.tools.lemmatizer.DefaultLemmatizerContextGenerator;\n\npublic class Example {\n    public static void main(String[] args) {\n        DefaultLemmatizerContextGenerator generator = new DefaultLemmatizerContextGenerator();\n        // API Usage Example\n    }\n}"}
{"api_description": "String[] tag(String[] sentence, Object[] additionalContext): class POSTagger\tAssigns the sentence of tokens pos tags.", "api_example": "import java.util.Arrays;\n\npublic class Main {\n    public static void main(String[] args) {\n        String[] sentence = {\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"};\n        Object[] additionalContext = null;\n\n        POSTagger tagger = new POSTagger();\n        String[] posTags = tagger.tag(sentence, additionalContext);\n\n        System.out.println(Arrays.toString(posTags));\n    }\n}\n\nclass POSTagger {\n    String[] tag(String[] sentence, Object[] additionalContext) {\n        // implementation of pos tagging logic\n        return new String[]{\"DT\", \"JJ\", \"JJ\", \"NN\", \"VBZ\"};\n    }\n}"}
{"api_description": "NewlineSentenceDetector(): class NewlineSentenceDetector", "api_example": "import edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.util.CoreMap;\nimport java.util.List;\n\npublic class Example {\n    public static void main(String[] args) {\n        StanfordCoreNLP pipeline = new StanfordCoreNLP();\n        \n        String text = \"This is a sentence. And another one.\";\n\n        Annotation document = new Annotation(text);\n\n        pipeline.annotate(document);\n\n        List<CoreMap> sentences = document.get(CoreAnnotations.SentencesAnnotation.class);\n\n        for (CoreMap sentence : sentences) {\n            System.out.println(sentence.toString());\n        }\n    }\n}"}
{"api_description": "Span[] sentPosDetect(CharSequence s): class NewlineSentenceDetector\tDetects sentences in a character sequence.", "api_example": "import opennlp.tools.sentdetect.NewlineSentenceDetector;\nimport opennlp.tools.sentdetect.SentenceDetectorME;\nimport opennlp.tools.util.Span;\n\npublic class SentenceDetectionExample {\n\n    public static void main(String[] args) {\n        NewlineSentenceDetector detector = new NewlineSentenceDetector();\n        CharSequence text = \"This is a sentence. And another one.\";\n        Span[] sentences = detector.sentPosDetect(text);\n        for (Span sentence : sentences) {\n            System.out.println(sentence.getCoveredText(text));\n        }\n    }\n}"}
{"api_description": "String[] tag(String[] sentence): class POSTagger\tAssigns the sentence of tokens pos tags.", "api_example": "import java.util.Arrays;\n\npublic class Main {\n    public static void main(String[] args) {\n        POSTagger posTagger = new POSTagger();\n        String[] sentence = {\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"};\n        String[] posTags = posTagger.tag(sentence);\n        System.out.println(Arrays.toString(posTags));\n    }\n}\n\nclass POSTagger {\n    String[] tag(String[] sentence) {\n        // Implementation of tagging the sentence with pos tags\n        return new String[]{\"DT\", \"JJ\", \"JJ\", \"NN\", \"VBZ\", \"IN\", \"DT\", \"JJ\", \"NN\"};\n    }\n}"}
{"api_description": "String getArticleStart(): class PDFTextStripper\tReturns the string which will be used at the beginning of an article.", "api_example": "import org.apache.pdfbox.text.PDFTextStripper;\n\npublic class Main {\n    public static void main(String[] args) {\n        PDFTextStripper pdfTextStripper = new PDFTextStripper();\n        String articleStart = pdfTextStripper.getArticleStart();\n        System.out.println(\"Article start: \" + articleStart);\n    }\n}"}
{"api_description": "static PDDocument Loader.loadPDF(org.apache.pdfbox.io.RandomAccessRead randomAccessRead): class Loader\tParses a PDF.", "api_example": "import org.apache.pdfbox.io.RandomAccessRead;\nimport org.apache.pdfbox.pdmodel.PDDocument;\nimport org.apache.pdfbox.Loader;\n\npublic class Main {\n    public static void main(String[] args) {\n        try {\n            // Assuming randomAccessRead is initialized elsewhere\n            RandomAccessRead randomAccessRead = initializeRandomAccessRead();\n            \n            // Loading PDF using the Loader API\n            PDDocument document = Loader.loadPDF(randomAccessRead);\n            \n            // Further operations on the loaded PDF document\n            // ...\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n    \n    private static RandomAccessRead initializeRandomAccessRead() {\n        // Implementation to initialize RandomAccessRead, can be from file, etc.\n        return null;\n    }\n}"}
{"api_description": "void persist(): class NaiveBayesModelWriter\tWrites the perceptron model, using the AbstractModelWriter.writeUTF(String), AbstractModelWriter.writeDouble(double), or AbstractModelWriter.writeInt(int)} methods implemented by extending classes.", "api_example": "import java.io.*;\n\npublic class NaiveBayesModelWriter {\n\n    private String modelFilePath;\n    private double modelAccuracy;\n\n    public NaiveBayesModelWriter(String modelFilePath, double modelAccuracy) {\n        this.modelFilePath = modelFilePath;\n        this.modelAccuracy = modelAccuracy;\n    }\n\n    public void persist() {\n        // implementation to persist the Naive Bayes model using AbstractModelWriter\n    }\n}"}
{"api_description": "OpenNLP", "api_example": "import java.io.*;\n\npublic class NaiveBayesModelWriter {\n\n    private String modelFilePath;\n    private double modelAccuracy;\n\n    public NaiveBayesModelWriter(String modelFilePath, double modelAccuracy) {\n        this.modelFilePath = modelFilePath;\n        this.modelAccuracy = modelAccuracy;\n    }\n\n    public void persist() {\n        // implementation to persist the Naive Bayes model using AbstractModelWriter\n    }\n}"}
{"api_description": "String tokens[] = tokenizer.tokenize(\"A sentence to be tokenized. \"): class TokenizerME\t;", "api_example": "import opennlp.tools.tokenize.TokenizerME;\nimport opennlp.tools.tokenize.TokenizerModel;\nimport opennlp.tools.util.Span;\n\npublic class TokenizerExample {\n\n    public static void main(String[] args) {\n        String sentence = \"A sentence to be tokenized.\";\n\n        TokenizerModel model = null; // initialize with the actual model\n\n        TokenizerME tokenizer = new TokenizerME(model);\n\n        String tokens[] = tokenizer.tokenize(sentence);\n    }\n}"}
{"api_description": "Note: Both, existing (binary): class ModelParameterChunker\tmodel files and newly trained models which don't require the chunking  technique, will be supported like in previous OpenNLP versions.", "api_example": "import java.io.FileInputStream;\nimport opennlp.tools.chunker.ChunkerME;\nimport opennlp.tools.chunker.ChunkerModel;\nimport opennlp.tools.util.Span;\n\npublic class ChunkerExample {\n\n    public static void main(String[] args) {\n        try {\n            FileInputStream modelIn = new FileInputStream(\"en-chunker.bin\");\n            ChunkerModel model = new ChunkerModel(modelIn);\n            ChunkerME chunker = new ChunkerME(model);\n\n            String[] sentence = new String[]{\"Rocky\", \"wants\", \"to\", \"play\", \"soccer\"};\n            String[] tags = new String[]{\"NNP\", \"VBZ\", \"TO\", \"VB\", \"NN\"};\n            Span[] chunks = chunker.chunkAsSpans(sentence, tags);\n\n            for (Span s : chunks) {\n                System.out.println(s);\n            }\n\n            modelIn.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}"}
{"api_description": "DictionaryLemmatizer(InputStream dictionaryStream): class DictionaryLemmatizer\tInitializes a DictionaryLemmatizer and related HashMap from the input tab separated dictionary.", "api_example": "import java.io.InputStream;\n\npublic class DictionaryLemmatizer {\n    private InputStream dictionaryStream;\n\n    public DictionaryLemmatizer(InputStream dictionaryStream) {\n        this.dictionaryStream = dictionaryStream;\n        // Add initialization code here\n    }\n\n    // Add other methods and functionality here\n}"}
{"api_description": "static String[] encodeLemmas(String[] toks, String[] lemmas): class LemmatizerME\tEncodes the word given its lemmas.", "api_example": "import opennlp.tools.lemmatizer.LemmatizerME;\n\npublic class Main {\n    public static void main(String[] args) {\n        String[] toks = {\"I\", \"am\", \"running\"};\n        String[] lemmas = {\"I\", \"be\", \"run\"};\n\n        String[] encodedToks = LemmatizerME.encodeLemmas(toks, lemmas);\n\n        for (String token : encodedToks) {\n            System.out.println(token);\n        }\n    }\n}"}
{"api_description": "String[] sentDetect(CharSequence s): class NewlineSentenceDetector\tDetects sentences in a character sequence.", "api_example": "import opennlp.tools.sentdetect.NewlineSentenceDetector;\n\npublic class SentenceDetectorExample {\n    public static void main(String[] args) {\n        NewlineSentenceDetector detector = new NewlineSentenceDetector();\n        String text = \"This is a sample sentence. And another one.\";\n\n        String[] sentences = detector.sentDetect(text);\n        for (String sentence : sentences) {\n            System.out.println(sentence);\n        }\n    }\n}"}
{"api_description": "TokenFeatureGeneratorFactory(): class TokenFeatureGeneratorFactory", "api_example": "import opennlp.tools.util.featuregen.TokenFeatureGeneratorFactory;\n\npublic class Main {\n    public static void main(String[] args) {\n        TokenFeatureGeneratorFactory tokenFeatureGeneratorFactory = new TokenFeatureGeneratorFactory();\n        // API usage example\n    }\n}"}
{"api_description": "String[] predictNextTokens(String... tokens): class LanguageModel\tPredict the most probable output sequence of tokens, given an input sequence of tokens.", "api_example": "import java.util.Arrays;\n\npublic class LanguageModel {\n    public static void main(String[] args) {\n        LanguageModel lm = new LanguageModel();\n        String[] inputTokens = {\"The\", \"quick\", \"brown\"};\n        \n        String[] predictedTokens = lm.predictNextTokens(inputTokens);\n        \n        System.out.println(Arrays.toString(predictedTokens));\n    }\n\n    String[] predictNextTokens(String... tokens) {\n        // Implementation here\n        return new String[]{\"fox\", \"jumps\", \"over\"};\n    }\n}"}
